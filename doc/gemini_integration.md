# GeminiとMCPサーバーの統合手順

このドキュメントでは、FastMCPサーバーを起動し、ローカルのPDFファイルをGeminiのような大規模言語モデル（LLM）が参照可能なリソースとして利用するための手順を説明します。

## 1. 前提条件

- Python 3.8以上がインストールされていること。
- `pip` が利用可能であること。
- 本プロジェクトの依存関係がインストールされていること（`requirements.txt`を参照）。

## 2. MCPサーバーの起動

MCPサーバーは、指定されたディレクトリ内のPDFファイルを読み込み、それらをMCPリソースとして公開します。

### 2.1. サーバーの実行コマンド

プロジェクトのルートディレクトリで、以下のコマンドを実行してサーバーを起動します。

```bash
PDF_DIR=/path/to/your/pdfs python src/main.py
```

**例:**

`/path/to/your/pdfs` ディレクトリにPDFファイルがある場合：

```bash
PDF_DIR=/path/to/your/pdfs python src/main.py
```

サーバーが起動すると、指定されたディレクトリ内のPDFファイルをスキャンし、各PDFファイルからテキストを抽出してMCPリソースとして登録します。リソースIDは `pdf/{ファイル名}` の形式になります。

### 2.2. サーバーの動作

サーバーは、PDFのテキストコンテンツをMCPリソースとして公開します。例えば、`document.pdf` というファイルがあれば、`pdf/document.pdf` というリソースIDでその内容が利用可能になります。

## 3. GeminiからのMCPリソースの利用

GeminiのようなLLMがMCPサーバーによって公開されたリソースを利用するには、LLMがMCPプロトコルをサポートし、サーバーに接続するように設定されている必要があります。

### 3.1. MCPプロトコルとLLMの連携

MCP（Model Context Protocol）は、LLMが外部のツールやデータにアクセスするための標準的な方法を定義します。LLMがMCPサーバーと連携するには、以下の設定が必要です。

1.  **MCPサーバーへの接続設定**: LLMの実行環境または設定において、MCPサーバーのエンドポイント（通常はURL）を指定します。これにより、LLMはリソースの取得リクエストをサーバーに送信できるようになります。

2.  **リソースの参照**: LLMは、プロンプトや内部ロジックを通じて、MCPサーバーが公開しているリソースID（例: `pdf/document.pdf`）を参照します。LLMは、このリソースIDを使用してサーバーにリクエストを送信し、対応するPDFの内容を取得します。

**例（概念）:**

LLMへのプロンプトで、以下のようにリソースを参照する場合があります。

`「pdf/document.pdf」の内容に基づいて、この文書の要約を作成してください。`

LLMは、このプロンプトを解析し、MCPサーバーに対して `pdf/document.pdf` の内容を要求します。サーバーはPDFから抽出したテキストをLLMに返し、LLMはそのテキストを利用して要約を生成します。

### 3.2. 注意事項

-   **LLMのMCPサポート**: Geminiが直接MCPをサポートしているかどうかは、GeminiのAPIドキュメントや機能に依存します。現時点では、LLMがMCPを直接サポートしているとは限りません。多くの場合、LLMとMCPサーバーの間に、MCPリクエストをLLMが理解できる形式に変換する中間層（エージェントやカスタムツール）が必要になることがあります。
-   **セキュリティとアクセス制御**: 本サーバーはローカルファイルシステムにアクセスするため、セキュリティ上の考慮が必要です。本番環境で使用する場合は、適切な認証、認可、およびネットワークセキュリティ対策を講じる必要があります。
-   **リソースのライフサイクル**: サーバーが停止すると、公開されていたリソースは利用できなくなります。永続的なリソースアクセスが必要な場合は、サーバーを常時稼働させるか、より堅牢なMCPサーバーソリューションを検討する必要があります。